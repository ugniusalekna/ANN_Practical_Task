{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMr6oY3WzOj1WA3jrLa1ORQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split"],"metadata":{"id":"utJXl_F2bngr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_file_list(npy_dir, folder_name):\n","  file_list = sorted(os.listdir(os.path.join(npy_dir, folder_name)))\n","  return file_list"],"metadata":{"id":"4aOat8x5eRp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def join_files(velocity_files, pressure_files):\n","  joined_files = []\n","  for u_file in velocity_files:\n","\n","    u_index = u_file.split('_')[-1].split('.')[0]  # Assuming the format 'velocity_X.npy'\n","    corresponding_p_file = next((p for p in pressure_files if p.split('_')[-1].split('.')[0] == u_index), None)\n","\n","    if corresponding_p_file:\n","      joined_files.append((u_file, corresponding_p_file))\n","\n","  return joined_files"],"metadata":{"id":"j9xfPpZpn-az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_joined_files(velocity_dir, pressure_dir, folder_name):\n","\n","  file_list_u_true = get_file_list(velocity_dir, folder_name)\n","  file_list_p_true = get_file_list(pressure_dir, folder_name)\n","\n","  return join_files(file_list_u_true, file_list_p_true)"],"metadata":{"id":"EiwZgwnDumS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FlowFieldDataset(Dataset):\n","  def __init__(self, velocity_dir, pressure_dir, files_true, files_noisy):\n","\n","    self.velocity_dir = velocity_dir\n","    self.pressure_dir = pressure_dir\n","    self.data_true = [self.load_one_instance(velocity_dir, pressure_dir, f, 'true_data') for f in files_true]\n","    self.data_noisy = [self.load_one_instance(velocity_dir, pressure_dir, f, 'noisy_data') for f in files_noisy]\n","\n","  def load_one_instance(self, velocity_dir, pressure_dir, file_tuple, data_type):\n","    u_file, p_file = file_tuple\n","    u_data = np.load(os.path.join(velocity_dir, data_type, u_file))\n","    p_data = np.load(os.path.join(pressure_dir, data_type, p_file))\n","\n","    return torch.from_numpy(u_data).float(), torch.from_numpy(p_data).float()\n","\n","  def __len__(self):\n","        return len(self.data_true)\n","\n","  def __getitem__(self, idx):\n","    u_hr_tensor, p_hr_tensor = self.data_true[idx]\n","    u_lr_tensor, p_lr_tensor = self.data_noisy[idx]\n","\n","    # Reorder the dimensions: [height, width, channels] to [channels, height, width]\n","    u_hr_tensor = u_hr_tensor.permute(2, 0, 1)\n","    p_hr_tensor = p_hr_tensor.unsqueeze(0)\n","    u_lr_tensor = u_lr_tensor.permute(2, 0, 1)\n","    p_lr_tensor = p_lr_tensor.unsqueeze(0)\n","\n","    return (u_hr_tensor, p_hr_tensor), (u_lr_tensor, p_lr_tensor)"],"metadata":{"id":"SNTa36hUhxsg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_test_split(dataset, test_size=0.2):\n","\n","  total_samples = len(dataset)\n","  test_sample_size = int(test_size * total_samples)\n","  train_sample_size = total_samples - test_sample_size\n","\n","  train_dataset, test_dataset = random_split(dataset, [train_sample_size, test_sample_size])\n","\n","  return train_dataset, test_dataset"],"metadata":{"id":"uCER_YahGVKc"},"execution_count":null,"outputs":[]}]}