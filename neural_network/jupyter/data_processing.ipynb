{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP1vCneALy0/fZnTxT3ENIE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split"],"metadata":{"id":"utJXl_F2bngr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FlowFieldDataset(Dataset):\n","  def __init__(self, velocity_dir, pressure_dir):\n","    self.velocity_dir = velocity_dir\n","    self.pressure_dir = pressure_dir\n","    self.files_true = self.get_joined_files('true_data')\n","    self.files_noisy = self.get_joined_files('noisy_data')\n","\n","  def load_one_instance(self, velocity_dir, pressure_dir, file_tuple, data_type):\n","    u_file, p_file = file_tuple\n","    u_data = np.load(os.path.join(velocity_dir, data_type, u_file))\n","    p_data = np.load(os.path.join(pressure_dir, data_type, p_file))\n","\n","    return torch.from_numpy(u_data).float(), torch.from_numpy(p_data).float()\n","\n","  def __len__(self):\n","    return len(self.files_true)\n","\n","  def __getitem__(self, idx):\n","    u_hr_tensor, p_hr_tensor = self.load_one_instance(self.velocity_dir, self.pressure_dir, self.files_true[idx], 'true_data')\n","    u_lr_tensor, p_lr_tensor = self.load_one_instance(self.velocity_dir, self.pressure_dir, self.files_noisy[idx], 'noisy_data')\n","\n","    # Reorder [height, width, channels] to [channels, height, width]\n","    u_hr_tensor = u_hr_tensor.permute(2, 0, 1)\n","    p_hr_tensor = p_hr_tensor.unsqueeze(0)\n","    u_lr_tensor = u_lr_tensor.permute(2, 0, 1)\n","    p_lr_tensor = p_lr_tensor.unsqueeze(0)\n","\n","    return (u_hr_tensor, p_hr_tensor), (u_lr_tensor, p_lr_tensor)\n","\n","  def join_files(self, velocity_files, pressure_files):\n","    joined_files = []\n","    for u_file in velocity_files:\n","      u_index = u_file.split('_')[-1].split('.')[0]\n","      corresponding_p_file = next((p for p in pressure_files if p.split('_')[-1].split('.')[0] == u_index), None)\n","      if corresponding_p_file:\n","        joined_files.append((u_file, corresponding_p_file))\n","    return joined_files\n","\n","  def get_joined_files(self, folder_name):\n","    velocity_files = sorted(os.listdir(os.path.join(self.velocity_dir, folder_name)))\n","    pressure_files = sorted(os.listdir(os.path.join(self.pressure_dir, folder_name)))\n","    return self.join_files(velocity_files, pressure_files)"],"metadata":{"id":"EGZcWkR62LKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_test_split(dataset, test_size=0.2):\n","  total_samples = len(dataset)\n","  test_sample_size = int(test_size * total_samples)\n","  train_sample_size = total_samples - test_sample_size\n","\n","  train_dataset, test_dataset = random_split(dataset, [train_sample_size, test_sample_size])\n","\n","  return train_dataset, test_dataset"],"metadata":{"id":"uCER_YahGVKc"},"execution_count":null,"outputs":[]}]}