{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMC6JpQg/vcFIR+FSVWshhK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install ipython==7.34.0 ipykernel==5.5.6\n","!pip install import_ipynb"],"metadata":{"id":"eO4KTMgm3bXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import os\n","import csv\n","import time\n","import torch\n","from scipy.ndimage import gaussian_filter, zoom\n","from skimage.filters import threshold_otsu\n","from sklearn.metrics import mean_squared_error\n","from skimage.metrics import structural_similarity as ssim\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import import_ipynb\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/Physics-Informed Neural Networks/Demo/fenics_cfd/neural_network'\n","os.chdir(path)\n","\n","from visualize_data import plot_numpy_matrices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-yy5EUNBrc4","executionInfo":{"status":"ok","timestamp":1704493756930,"user_tz":-120,"elapsed":2388,"user":{"displayName":"Ugnius","userId":"13334014540853860838"}},"outputId":"0306c765-9a49-4c6e-b3e3-220c4f78464a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","importing Jupyter notebook from /content/drive/MyDrive/Colab Notebooks/Physics-Informed Neural Networks/Demo/fenics_cfd/neural_network/visualize_data.ipynb\n"]}]},{"cell_type":"code","source":["def to_numpy(tensor):\n","  return tensor.detach().cpu().numpy()"],"metadata":{"id":"EtbNgImc3JwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_unique_filename(base_path, base_name, ext):\n","    counter = 1\n","    while True:\n","        file_name = f\"{base_name}_{counter}.{ext}\"\n","        file_path = os.path.join(base_path, file_name)\n","        if not os.path.exists(file_path):\n","            return file_path\n","        counter += 1"],"metadata":{"id":"xAs5NTV1oXAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def monitor_gpu_memory():\n","    if torch.cuda.is_available():\n","        allocated = torch.cuda.memory_allocated()\n","        cached = torch.cuda.memory_reserved()\n","        return allocated / (1024 ** 2), cached / (1024 ** 2)  # to megabytes\n","    else:\n","        return 0, 0"],"metadata":{"id":"0I1sIE4VoZVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_meshgrid(height, width, device, num_sampled_points=None):\n","\n","    y_coords, x_coords = torch.meshgrid(torch.linspace(0, height, 280), torch.linspace(0, width, 280), indexing='xy')\n","    coords = torch.stack([x_coords, y_coords], dim=0)\n","\n","    if num_sampled_points == None:\n","        coords = coords.to(device)\n","        return coords\n","\n","    flat_coords = coords.view(2, -1).transpose(0, 1)  # Shape: [280*280, 2]\n","\n","    indices = torch.randperm(flat_coords.size(0))[:num_sampled_points]\n","    sampled_flat_coords = torch.zeros_like(flat_coords)\n","    sampled_flat_coords[indices] = flat_coords[indices]\n","\n","    sampled_coords = sampled_flat_coords.transpose(0, 1).view(2, 280, 280).unsqueeze(0)\n","    sampled_coords = sampled_coords.to(device)\n","\n","    return sampled_coords"],"metadata":{"id":"Ks2LYRzZ1vYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_vessel_otsu(velocity_field, pressure_field, gamma, sigma, target_size, device):\n","    def apply_blur(field, gamma, sigma):\n","        field_transformed = np.maximum(field, 0) ** gamma\n","        return gaussian_filter(field_transformed / np.max(field_transformed), sigma=sigma)\n","\n","    def upscale(field, target_size):\n","        zoom_factors = [target_size[i] / field.shape[i+2] for i in range(2)]\n","        return zoom(field, zoom=[1, 1, *zoom_factors], order=3)  # Bicubic interpolation\n","\n","    velocity_np = to_numpy(velocity_field)\n","    pressure_np = to_numpy(pressure_field)\n","\n","    if velocity_np.shape[2:] != target_size:\n","        velocity_np = upscale(velocity_np, target_size)\n","\n","    if pressure_np.shape[2:] != target_size:\n","        pressure_np = upscale(pressure_np, target_size)\n","\n","    velocity_magnitude = np.linalg.norm(velocity_np, axis=1)\n","    blurred_velocity = apply_blur(velocity_magnitude, gamma, sigma)\n","    blurred_pressure = apply_blur(pressure_np.squeeze(1), gamma, sigma)\n","\n","    velocity_mask = (blurred_velocity > threshold_otsu(blurred_velocity)).astype(np.float32)\n","    pressure_mask = (blurred_pressure > threshold_otsu(blurred_pressure)).astype(np.float32)\n","\n","    combined_mask = np.maximum(velocity_mask[:, np.newaxis, :, :], pressure_mask[:, np.newaxis, :, :])\n","\n","    return torch.from_numpy(combined_mask).float().to(device)"],"metadata":{"id":"YWLu3isf3F74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_derivative(tensor, dx, dy, order=1):\n","    derivative_x = torch.zeros_like(tensor)\n","    derivative_y = torch.zeros_like(tensor)\n","\n","    # Central differences in the interior\n","    if order == 1:\n","        derivative_x[:, :, 1:-1, 1:-1] = (tensor[:, :, 2:, 1:-1] - tensor[:, :, :-2, 1:-1]) / (2 * dx)\n","        derivative_y[:, :, 1:-1, 1:-1] = (tensor[:, :, 1:-1, 2:] - tensor[:, :, 1:-1, :-2]) / (2 * dy)\n","    elif order == 2:\n","        derivative_x[:, :, 1:-1, 1:-1] = (tensor[:, :, 2:, 1:-1] - 2 * tensor[:, :, 1:-1, 1:-1] + tensor[:, :, :-2, 1:-1]) / (dx ** 2)\n","        derivative_y[:, :, 1:-1, 1:-1] = (tensor[:, :, 1:-1, 2:] - 2 * tensor[:, :, 1:-1, 1:-1] + tensor[:, :, 1:-1, :-2]) / (dy ** 2)\n","\n","    # Forward differences for the first row/column\n","    if order == 1:\n","        derivative_x[:, :, 0, :] = (tensor[:, :, 1, :] - tensor[:, :, 0, :]) / dx\n","        derivative_y[:, :, :, 0] = (tensor[:, :, :, 1] - tensor[:, :, :, 0]) / dy\n","    elif order == 2:\n","        derivative_x[:, :, 0, :] = (tensor[:, :, 2, :] - 2 * tensor[:, :, 1, :] + tensor[:, :, 0, :]) / (dx ** 2)\n","        derivative_y[:, :, :, 0] = (tensor[:, :, :, 2] - 2 * tensor[:, :, :, 1] + tensor[:, :, :, 0]) / (dy ** 2)\n","\n","    # Backward differences for the last row/column\n","    if order == 1:\n","        derivative_x[:, :, -1, :] = (tensor[:, :, -1, :] - tensor[:, :, -2, :]) / dx\n","        derivative_y[:, :, :, -1] = (tensor[:, :, :, -1] - tensor[:, :, :, -2]) / dy\n","    elif order == 2:\n","        derivative_x[:, :, -1, :] = (tensor[:, :, -1, :] - 2 * tensor[:, :, -2, :] + tensor[:, :, -3, :]) / (dx ** 2)\n","        derivative_y[:, :, :, -1] = (tensor[:, :, :, -1] - 2 * tensor[:, :, :, -2] + tensor[:, :, :, -3]) / (dy ** 2)\n","\n","    return derivative_x, derivative_y"],"metadata":{"id":"aArv-UE51GRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_navier_stokes_loss(u_pred, p_pred, coordinates, mask, rho, mu, device, norm='L2', autograd=False):\n","\n","    u, v, p = u_pred[:, [0]] * mask, u_pred[:, [1]] * mask, p_pred * mask\n","\n","    # Plot mask and masked regions\n","    # vector_field = torch.cat((u, v), dim=1)\n","    # for j in range(u.shape[0]):\n","    #     plot_numpy_matrices(to_numpy(vector_field[j]), to_numpy(p[j][0]))\n","    #     plot_with_transparent_mask(to_numpy(vector_field[j]), to_numpy(mask[j][0]))\n","\n","    if autograd: # Computational graph derivatives\n","\n","        du_dxy = torch.autograd.grad(u, coordinates, grad_outputs=torch.ones_like(u).to(device), retain_graph=True, create_graph=True)[0]\n","        dv_dxy = torch.autograd.grad(v, coordinates, grad_outputs=torch.ones_like(v).to(device), retain_graph=True, create_graph=True)[0]\n","        dp_dxy = torch.autograd.grad(p, coordinates, grad_outputs=torch.ones_like(p).to(device), retain_graph=True, create_graph=True)[0]\n","\n","        d2u_dxy2 = torch.autograd.grad(du_dxy, coordinates, grad_outputs=torch.ones_like(du_dxy).to(device), create_graph=True)[0]\n","        d2v_dxy2 = torch.autograd.grad(dv_dxy, coordinates, grad_outputs=torch.ones_like(dv_dxy).to(device), create_graph=True)[0]\n","\n","        du_dx, du_dy = du_dxy[:, [0]], du_dxy[:, [1]]\n","        dv_dx, dv_dy = dv_dxy[:, [0]], dv_dxy[:, [1]]\n","        dp_dx, dp_dy = dp_dxy[:, [0]], dp_dxy[:, [1]]\n","        d2u_dx2, d2u_dy2 = d2u_dxy2[:, [0]], d2u_dxy2[:, [1]]\n","        d2v_dx2, d2v_dy2 = d2v_dxy2[:, [0]], d2v_dxy2[:, [1]]\n","\n","    else: # Finite differences\n","\n","        dx, dy = 0.006 / u.shape[2], 0.006 / u.shape[3]\n","\n","        du_dx, du_dy = compute_derivative(u, dx, dy, order=1)\n","        dv_dx, dv_dy = compute_derivative(v, dx, dy, order=1)\n","        dp_dx, dp_dy = compute_derivative(p, dx, dy, order=1)\n","\n","        d2u_dx2, d2u_dy2 = compute_derivative(u, dx, dy, order=2)\n","        d2v_dx2, d2v_dy2 = compute_derivative(v, dx, dy, order=2)\n","\n","    continuity = du_dx + dv_dy\n","\n","    momentum_u = rho * (u * du_dx + v * du_dy) + dp_dx - mu * (d2u_dx2 + d2u_dy2)\n","    momentum_v = rho * (u * dv_dx + v * dv_dy) + dp_dy - mu * (d2v_dx2 + d2v_dy2)\n","\n","    match norm:\n","        case 'L1':\n","            continuity_loss = torch.sum(torch.abs(continuity))\n","            momentum_loss = torch.sum(torch.abs(momentum_u)) + torch.sum(torch.abs(momentum_v))\n","        case 'L2':\n","            continuity_loss = torch.sqrt(torch.sum(continuity**2))\n","            momentum_loss = torch.sqrt(torch.sum(momentum_u**2)) + torch.sqrt(torch.sum(momentum_v**2))\n","        case 'Linf':\n","            continuity_loss = torch.max(torch.abs(continuity))\n","            momentum_loss = torch.max(torch.abs(momentum_u)) + torch.max(torch.abs(momentum_v))\n","\n","    physics_loss = continuity_loss + momentum_loss\n","\n","    return physics_loss / 1000"],"metadata":{"id":"8QCcVz0GVL-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, optimizer, criterion, num_epochs, device, alpha=0.5,\n","                model_save_path=None, loss_save_path=None, log_interval=10, is_physics_informed=False, norm='L2'):\n","    if device.type == 'cuda':\n","        torch.cuda.empty_cache()\n","    model.train()\n","\n","    all_data_losses = []\n","    all_physics_losses = []\n","\n","    model_file_base_name = 'PICNN_model_params' if is_physics_informed else 'CNN_model_params'\n","    loss_file_base_name = 'PICNN_train_losses' if is_physics_informed else 'CNN_train_losses'\n","\n","    if model_save_path is not None:\n","        os.makedirs(model_save_path, exist_ok=True)\n","        save_model_file = get_unique_filename(model_save_path, model_file_base_name, 'pt')\n","        print(f'Model param file name: {save_model_file}')\n","\n","    if loss_save_path is not None:\n","        os.makedirs(loss_save_path, exist_ok=True)\n","        save_loss_file = get_unique_filename(loss_save_path, loss_file_base_name, 'csv')\n","        print(f'Loss file name: {save_loss_file}')\n","\n","    with open(save_loss_file, 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Epoch', 'Step', 'Data Loss', 'Physics Loss' if is_physics_informed else '', 'Loss'])\n","\n","        total_start_time = time.time()\n","\n","        for epoch in range(num_epochs):\n","            epoch_start_time = time.time()\n","\n","            running_loss = 0.0\n","            running_data_loss = 0.0\n","            running_physics_loss = 0.0\n","\n","            for i, ((u_hr, p_hr), (u_lr, p_lr)) in enumerate(train_loader):\n","                u_hr, p_hr = u_hr.to(device), p_hr.to(device)\n","                u_lr, p_lr = u_lr.to(device), p_lr.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                if is_physics_informed:\n","                    if model.use_coords:\n","                        coordinates = create_meshgrid(0.006, 0.006, device, num_sampled_points=100)\n","                        coordinates = coordinates.expand(u_lr.size(0), -1, -1, -1)\n","                        coordinates.requires_grad_(True)\n","                    else:\n","                        coordinates = None\n","                    u_pred, p_pred = model(u_lr, p_lr, coordinates)\n","                    mask = segment_vessel_otsu(u_hr, p_hr, gamma=0.5, sigma=1.5, target_size=(280, 280), device=device)\n","                    data_loss = (criterion(u_pred, u_hr) + criterion(p_pred, p_hr))\n","                    physics_loss = compute_navier_stokes_loss(u_pred, p_pred, coordinates, mask, rho=1060, mu=0.0035, device=device, norm=norm, autograd=model.use_coords)\n","                    loss = (1 - alpha) * data_loss + alpha * physics_loss\n","                else:\n","                    u_pred, p_pred = model(u_lr, p_lr)\n","                    data_loss = (criterion(u_pred, u_hr) + criterion(p_pred, p_hr))\n","                    loss = data_loss\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","                running_loss += loss.item()\n","                running_data_loss += data_loss.item() if is_physics_informed else loss.item()\n","                running_physics_loss += physics_loss.item() if is_physics_informed else 0\n","\n","                if model_save_path is not None and (epoch * len(train_loader) + i) % 1000 == 0:\n","                    allocated_memory_MB, cached_memory_MB = monitor_gpu_memory()\n","                    print(f\"Allocated Memory: {allocated_memory_MB:.2f} MB, Cached Memory: {cached_memory_MB:.2f} MB\")\n","                    if i > 0:\n","                        print(f'Should be saving now!')\n","                        torch.save(model.state_dict(), save_model_file)\n","\n","                        # Plot progress every 1000 iterations\n","                        # u_lr_np, p_lr_np = to_numpy(u_lr), to_numpy(p_lr)\n","                        # u_hr_np, p_hr_np = to_numpy(u_hr), to_numpy(p_hr)\n","                        # u_pred_np, p_pred_np = to_numpy(u_pred), to_numpy(p_pred)\n","                        # for j in range(u_pred_np.shape[0]):\n","                        #     plot_numpy_matrices(u_lr_np[j], p_lr_np[j][0], main_title=f\"Noisy: Epoch {epoch}, step {i}\", plot_size=6)\n","                        #     plot_numpy_matrices(u_hr_np[j], p_hr_np[j][0], main_title=f\"True: Epoch {epoch}, step {i}\", plot_size=6)\n","                        #     plot_numpy_matrices(u_pred_np[j], p_pred_np[j][0], main_title=f\"Predicted: Epoch {epoch}, step {i}\", plot_size=6)\n","\n","                if (i + 1) % log_interval == 0:\n","                    writer.writerow([epoch, i, data_loss.item(), physics_loss.item() if is_physics_informed else '', loss.item()])\n","                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Total Loss: {running_loss / log_interval:.6f}, Data Loss: {running_data_loss / log_interval:.6f}, Physics Loss: {running_physics_loss / log_interval:.6f}')\n","                    all_data_losses.append(running_data_loss / log_interval)\n","                    if is_physics_informed:\n","                        all_physics_losses.append(running_physics_loss / log_interval)\n","\n","                    running_loss = 0.0\n","                    running_data_loss = 0.0\n","                    running_physics_loss = 0.0\n","\n","            epoch_duration = time.time() - epoch_start_time\n","            print(f'Epoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f} s')\n","\n","    total_duration = time.time() - total_start_time\n","    print(f'Total training time: {total_duration:.2f} s')\n","\n","    # Final save\n","    if model_save_path is not None:\n","        torch.save(model.state_dict(), save_model_file)\n","        print(f'Model params saved to: {save_model_file}')\n","        print(f'Losses saved to: {save_loss_file}')\n","\n","\n","    if is_physics_informed:\n","        return all_data_losses, all_physics_losses\n","    else:\n","        return all_data_losses"],"metadata":{"id":"aWOOBetb2eki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_model(model, test_loader, criterion, device, test_loss_save_path=None, log_interval=10, is_physics_informed=False):\n","    model.eval()\n","    total_loss = 0\n","    results = []\n","\n","    if test_loss_save_path is not None:\n","        loss_file_base_name = 'PICNN_test_losses' if is_physics_informed else 'CNN_test_losses'\n","        os.makedirs(test_loss_save_path, exist_ok=True)\n","        save_loss_file = get_unique_filename(test_loss_save_path, loss_file_base_name, 'csv')\n","\n","    with torch.no_grad(), open(save_loss_file, 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Step', 'Loss'])\n","\n","        for i, ((u_hr, p_hr), (u_lr, p_lr)) in enumerate(test_loader):\n","            u_hr, p_hr = u_hr.to(device), p_hr.to(device)\n","            u_lr, p_lr = u_lr.to(device), p_lr.to(device)\n","\n","            if is_physics_informed:\n","                if model.use_coords:\n","                    coordinates = create_meshgrid(0.006, 0.006, device, num_sampled_points=100)\n","                    coordinates = coordinates.expand(u_lr.size(0), -1, -1, -1)\n","                else:\n","                    coordinates = None\n","                u_pred, p_pred = model(u_lr, p_lr, coordinates)\n","                loss = (100 * criterion(u_pred, u_hr) + criterion(p_pred, p_hr))\n","\n","            else:\n","                u_pred, p_pred = model(u_lr, p_lr)\n","                loss = (100 * criterion(u_pred, u_hr) + criterion(p_pred, p_hr))\n","\n","            total_loss += loss\n","\n","            if (i + 1) % log_interval == 0:\n","                writer.writerow([i, loss.item()])\n","                print(f'Step [{i+1}/{len(test_loader)}], Data Loss: {loss.item():.6f}')\n","\n","            results.append({\n","                'noisy': (u_lr.cpu().numpy(), p_lr.cpu().numpy()),\n","                'predicted': (u_pred.cpu().numpy(), p_pred.cpu().numpy()),\n","                'true': (u_hr.cpu().numpy(), p_hr.cpu().numpy())\n","            })\n","\n","    avg_loss = total_loss / len(test_loader)\n","    print(f'Average Test Loss: {avg_loss:.6f}')\n","\n","    return to_numpy(avg_loss), results"],"metadata":{"id":"oUtJLI6G3VH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, test_loader, device, is_physics_informed=False):\n","    model.eval()\n","\n","    def psnr(y_true, y_pred):\n","        mse = mean_squared_error(y_true, y_pred)\n","        max_pixel = 1.0  # Assuming pixel values range from 0 to 1\n","        return 20 * np.log10(max_pixel / np.sqrt(mse))\n","\n","    total_mse_velocity = total_psnr_velocity = total_ssim_velocity = 0\n","    total_mse_pressure = total_psnr_pressure = total_ssim_pressure = 0\n","\n","    with torch.no_grad():\n","        for (u_hr, p_hr), (u_lr, p_lr) in test_loader:\n","            u_hr, p_hr = u_hr.to(device), p_hr.to(device)\n","            u_lr, p_lr = u_lr.to(device), p_lr.to(device)\n","\n","            if is_physics_informed:\n","                if model.use_coords:\n","                    coordinates = create_meshgrid(0.006, 0.006, device, num_sampled_points=100)\n","                    coordinates = coordinates.expand(u_lr.size(0), -1, -1, -1)\n","                else:\n","                    coordinates=None\n","                u_pred, p_pred = model(u_lr, p_lr, coordinates)\n","            else:\n","                u_pred, p_pred = model(u_lr, p_lr)\n","\n","            u_hr_np, u_pred_np = to_numpy(u_hr), to_numpy(u_pred)\n","            p_hr_np, p_pred_np = to_numpy(p_hr), to_numpy(p_pred)\n","\n","            for i in range(u_hr_np.shape[0]): # one batch\n","\n","                u_hr_batch, u_pred_batch = u_hr_np[i], u_pred_np[i]\n","                for j in range(u_hr_batch.shape[0]):\n","                    total_mse_velocity += mean_squared_error(u_hr_batch[j], u_pred_batch[j])\n","                    total_psnr_velocity += psnr(u_hr_batch[j], u_pred_batch[j])\n","                    total_ssim_velocity += ssim(u_hr_batch[j], u_pred_batch[j], data_range=u_hr_batch[j].max()-u_hr_batch[j].min())\n","\n","                p_hr_batch, p_pred_batch = p_hr_np[i].squeeze(0), p_pred_np[i].squeeze(0)\n","                total_mse_pressure += mean_squared_error(p_hr_batch, p_pred_batch)\n","                total_psnr_pressure += psnr(p_hr_batch, p_pred_batch)\n","                total_ssim_pressure += ssim(p_hr_batch, p_pred_batch, data_range=p_hr_batch.max()-p_hr_batch.min())\n","\n","\n","    avg_mse_velocity = total_mse_velocity / len(test_loader)\n","    avg_psnr_velocity = total_psnr_velocity / len(test_loader)\n","    avg_ssim_velocity = total_ssim_velocity / len(test_loader)\n","\n","    avg_mse_pressure = total_mse_pressure / len(test_loader)\n","    avg_psnr_pressure = total_psnr_pressure / len(test_loader)\n","    avg_ssim_pressure = total_ssim_pressure / len(test_loader)\n","\n","    velocity_metrics = [avg_mse_velocity, avg_psnr_velocity, avg_ssim_velocity]\n","    pressure_metrics = [avg_mse_pressure, avg_psnr_pressure, avg_ssim_pressure]\n","\n","    return velocity_metrics, pressure_metrics"],"metadata":{"id":"7YuP62YP4yCD"},"execution_count":null,"outputs":[]}]}