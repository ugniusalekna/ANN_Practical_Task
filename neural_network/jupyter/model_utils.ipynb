{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO0fA8zrBjx2uyDUgPiLEiN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install ipython==7.34.0 ipykernel==5.5.6\n","!pip install import_ipynb"],"metadata":{"id":"eO4KTMgm3bXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.ndimage import gaussian_filter, zoom\n","from skimage.filters import threshold_otsu\n","from sklearn.metrics import mean_squared_error\n","from skimage.metrics import structural_similarity as ssim\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","import import_ipynb\n","import imageio\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/Physics-Informed Neural Networks/Demo/fenics_cfd/neural_network'\n","os.chdir(path)\n","\n","from visualize_geometry import plot_numpy_matrices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-yy5EUNBrc4","executionInfo":{"status":"ok","timestamp":1704213368827,"user_tz":-120,"elapsed":21948,"user":{"displayName":"Ugnius","userId":"13334014540853860838"}},"outputId":"36ef61ae-9f8a-44c5-d814-dbce8c9c414c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","importing Jupyter notebook from visualize_geometry.ipynb\n"]}]},{"cell_type":"code","source":["def to_numpy(tensor):\n","  return tensor.detach().cpu().numpy()"],"metadata":{"id":"EtbNgImc3JwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_meshgrid(height, width, device, num_sampled_points=None):\n","\n","  y_coords, x_coords = torch.meshgrid(torch.linspace(0, height, 280), torch.linspace(0, width, 280), indexing='xy')\n","  coords = torch.stack([x_coords, y_coords], dim=0)\n","\n","  if num_sampled_points == None:\n","    coords = coords.to(device)\n","    return coords.requires_grad_(True)\n","\n","  flat_coords = coords.view(2, -1).transpose(0, 1)  # Shape: [280*280, 2]\n","\n","  indices = torch.randperm(flat_coords.size(0))[:num_sampled_points]\n","  sampled_flat_coords = torch.zeros_like(flat_coords)\n","  sampled_flat_coords[indices] = flat_coords[indices]\n","\n","  sampled_coords = sampled_flat_coords.transpose(0, 1).view(2, 280, 280).unsqueeze(0)\n","  sampled_coords = sampled_coords.to(device)\n","\n","  return sampled_coords.requires_grad_(True)"],"metadata":{"id":"Ks2LYRzZ1vYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_vessel_otsu(velocity_field, pressure_field, gamma, sigma, target_size, device):\n","  def apply_blur(field, gamma, sigma):\n","    field_transformed = np.maximum(field, 0) ** gamma\n","    return gaussian_filter(field_transformed / np.max(field_transformed), sigma=sigma)\n","\n","  def upscale(field, target_size):\n","    zoom_factors = [target_size[i] / field.shape[i+2] for i in range(2)]\n","    return zoom(field, zoom=[1, 1, *zoom_factors], order=3)  # Bicubic interpolation\n","\n","  velocity_np = to_numpy(velocity_field)\n","  pressure_np = to_numpy(pressure_field)\n","\n","  if velocity_np.shape[2:] != target_size:\n","    velocity_np = upscale(velocity_np, target_size)\n","\n","  if pressure_np.shape[2:] != target_size:\n","    pressure_np = upscale(pressure_np, target_size)\n","\n","  velocity_magnitude = np.linalg.norm(velocity_np, axis=1)\n","  blurred_velocity = apply_blur(velocity_magnitude, gamma, sigma)\n","  blurred_pressure = apply_blur(pressure_np.squeeze(1), gamma, sigma)\n","\n","  velocity_mask = (blurred_velocity > threshold_otsu(blurred_velocity)).astype(np.float32)\n","  pressure_mask = (blurred_pressure > threshold_otsu(blurred_pressure)).astype(np.float32)\n","\n","  combined_mask = np.maximum(velocity_mask[:, np.newaxis, :, :], pressure_mask[:, np.newaxis, :, :])\n","\n","  return torch.from_numpy(combined_mask).float().to(device)"],"metadata":{"id":"YWLu3isf3F74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_navier_stokes_loss(u_pred, p_pred, coordinates, mask, rho, mu, device):\n","  u, v, p = u_pred[:, [0]] * mask, u_pred[:, [1]] * mask, p_pred * mask\n","\n","  du_dxy = torch.autograd.grad(u, coordinates, grad_outputs=torch.ones_like(u).to(device), retain_graph=True, create_graph=True)[0]\n","  dv_dxy = torch.autograd.grad(v, coordinates, grad_outputs=torch.ones_like(v).to(device), retain_graph=True, create_graph=True)[0]\n","  dp_dxy = torch.autograd.grad(p, coordinates, grad_outputs=torch.ones_like(p).to(device), retain_graph=True, create_graph=True)[0]\n","\n","  d2u_dxy2 = torch.autograd.grad(du_dxy, coordinates, grad_outputs=torch.ones_like(du_dxy).to(device), create_graph=True)[0]\n","  d2v_dxy2 = torch.autograd.grad(dv_dxy, coordinates, grad_outputs=torch.ones_like(dv_dxy).to(device), create_graph=True)[0]\n","\n","  du_dx, du_dy = du_dxy[:, [0]], du_dxy[:, [1]]\n","  dv_dx, dv_dy = dv_dxy[:, [0]], dv_dxy[:, [1]]\n","  dp_dx, dp_dy = dp_dxy[:, [0]], dp_dxy[:, [1]]\n","  d2u_dx2, d2u_dy2 = d2u_dxy2[:, [0]], d2u_dxy2[:, [1]]\n","  d2v_dx2, d2v_dy2 = d2v_dxy2[:, [0]], d2v_dxy2[:, [1]]\n","\n","  continuity = du_dx + dv_dy\n","  continuity_loss = torch.max(torch.abs(continuity))\n","\n","  momentum_u = rho * (u * du_dx + v * du_dy) + dp_dx - mu * (d2u_dx2 + d2u_dy2)\n","  momentum_v = rho * (u * dv_dx + v * dv_dy) + dp_dy - mu * (d2v_dx2 + d2v_dy2)\n","  momentum_loss = torch.max(torch.abs(momentum_u)) + torch.max(torch.abs(momentum_v))\n","\n","  physics_loss = continuity_loss + momentum_loss\n","\n","  return physics_loss / 1000"],"metadata":{"id":"aArv-UE51GRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, optimizer, criterion, num_epochs, device, alpha=0.5, save_path=None, is_physics_informed=False):\n","  if device.type == 'cuda':\n","    torch.cuda.empty_cache()\n","  model.train()\n","\n","  all_data_losses = []\n","  all_physics_losses = []\n","\n","  if save_path is not None:\n","    os.makedirs(save_path, exist_ok=True)\n","    save_file = os.path.join(save_path, 'model_params.pt')\n","\n","  for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    running_physics_loss = 0.0\n","\n","    for i, ((u_hr, p_hr), (u_lr, p_lr)) in enumerate(train_loader):\n","      u_hr, p_hr = u_hr.to(device), p_hr.to(device)\n","      u_lr, p_lr = u_lr.to(device), p_lr.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      if is_physics_informed:\n","        coordinates = create_meshgrid(0.006, 0.006, device, num_sampled_points=1000).requires_grad_(True)\n","        coordinates = coordinates.expand(u_lr.size(0), -1, -1, -1)\n","        u_pred, p_pred = model(u_lr, p_lr, coordinates)\n","        mask = segment_vessel_otsu(u_lr, p_lr, gamma=0.5, sigma=1.5, target_size=(280, 280), device=device)\n","        data_loss = (criterion(u_pred, u_hr) + criterion(p_pred, p_hr))\n","        physics_loss = compute_navier_stokes_loss(u_pred, p_pred, coordinates, mask, rho=1060, mu=0.0035, device=device)\n","        loss = (1 - alpha) * data_loss + alpha * physics_loss\n","      else:\n","        u_pred, p_pred = model(u_lr, p_lr)\n","        loss = (criterion(u_pred, u_hr) + criterion(p_pred, p_hr))\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      if is_physics_informed:\n","        running_physics_loss += physics_loss\n","\n","      save_interval=1000\n","      if save_path is not None and (epoch * len(train_loader) + i) % save_interval == 0:\n","        torch.save(model.state_dict(), save_file)\n","\n","      if (i + 1) % 10 == 0:\n","        if is_physics_informed:\n","          print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss / 10:.6f}, Physics Loss: {running_physics_loss / 10:.6f}')\n","        else:\n","          print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss / 10:.6f}')\n","          all_physics_losses.append(running_physics_loss / 10)\n","\n","        running_loss = 0.0\n","        running_physics_loss = 0.0\n","\n","    if save_path is not None:\n","      torch.save(model.state_dict(), save_file)\n","\n","  return all_data_losses, all_physics_losses if is_physics_informed else all_data_losses"],"metadata":{"id":"aWOOBetb2eki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_model(model, test_loader, criterion, device, num_visualizations=3, is_physics_informed=False):\n","  model.eval()\n","  total_loss = 0\n","  results = []\n","\n","  with torch.no_grad():\n","    for i, ((u_hr, p_hr), (u_lr, p_lr)) in enumerate(test_loader):\n","      u_hr, p_hr = u_hr.to(device), p_hr.to(device)\n","      u_lr, p_lr = u_lr.to(device), p_lr.to(device)\n","\n","      if is_physics_informed:\n","        coordinates = create_meshgrid(0.006, 0.006, device, num_sampled_points=1000)\n","        coordinates = coordinates.expand(u_lr.size(0), -1, -1, -1)\n","        u_pred, p_pred = model(u_lr, p_lr, coordinates)\n","      else:\n","        u_pred, p_pred = model(u_lr, p_lr)\n","\n","      total_loss += (criterion(u_pred, u_hr) + criterion(p_pred, p_hr)).item()\n","\n","      results.append({\n","        'noisy': (u_lr.cpu().numpy(), p_lr.cpu().numpy()),\n","        'predicted': (u_pred.cpu().numpy(), p_pred.cpu().numpy()),\n","        'true': (u_hr.cpu().numpy(), p_hr.cpu().numpy())\n","      })\n","\n","      if i < num_visualizations:\n","        u_pred_np, p_pred_np = to_numpy(u_pred), to_numpy(p_pred)\n","        u_hr_np, p_hr_np = to_numpy(u_hr), to_numpy(p_hr)\n","\n","        for j in range(u_pred_np.shape[0]):\n","          plot_numpy_matrices(u_hr_np[j].transpose(1, 2, 0), p_hr_np[j][0], main_title=\"True Flow Field\", plot_size=6)\n","          plot_numpy_matrices(u_pred_np[j].transpose(1, 2, 0), p_pred_np[j][0], main_title=\"Predicted Flow Field\", plot_size=6)\n","\n","  avg_loss = total_loss / len(test_loader)\n","  print(f'Test Loss: {avg_loss}')\n","\n","  return avg_loss, results"],"metadata":{"id":"oUtJLI6G3VH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, test_loader, device, is_physics_informed=False):\n","  model.eval()\n","\n","  def psnr(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    max_pixel = 1.0  # Assuming pixel values range from 0 to 1\n","    return 20 * np.log10(max_pixel / np.sqrt(mse))\n","\n","  total_mse_velocity = total_psnr_velocity = total_ssim_velocity = 0\n","  total_mse_pressure = total_psnr_pressure = total_ssim_pressure = 0\n","  num_samples = len(test_loader.dataset)\n","\n","  with torch.no_grad():\n","    for (u_hr, p_hr), (u_lr, p_lr) in test_loader:\n","      u_hr, p_hr = u_hr.to(device), p_hr.to(device)\n","      u_lr, p_lr = u_lr.to(device), p_lr.to(device)\n","\n","      if is_physics_informed:\n","        coordinates = create_meshgrid(0.006, 0.006, device, num_sampled_points=1000)\n","        u_pred, p_pred = model(u_lr, p_lr, coordinates)\n","      else:\n","        u_pred, p_pred = model(u_lr, p_lr)\n","\n","      u_hr_np, u_pred_np = to_numpy(u_hr).squeeze(0), to_numpy(u_pred).squeeze(0)\n","      p_hr_np, p_pred_np = to_numpy(p_hr).squeeze(0).squeeze(0), to_numpy(p_pred).squeeze(0).squeeze(0)\n","\n","      for i in range(u_hr_np.shape[0]):\n","        total_mse_velocity += mean_squared_error(u_hr_np[i], u_pred_np[i])\n","        total_psnr_velocity += psnr(u_hr_np[i], u_pred_np[i])\n","        total_ssim_velocity += ssim(u_hr_np[i], u_pred_np[i])\n","\n","        total_mse_pressure += mean_squared_error(p_hr_np[i], p_pred_np[i])\n","        total_psnr_pressure += psnr(p_hr_np[i], p_pred_np[i])\n","        total_ssim_pressure += ssim(p_hr_np[i], p_pred_np[i])\n","\n","  avg_mse_velocity = total_mse_velocity / len(test_loader)\n","  avg_psnr_velocity = total_psnr_velocity / len(test_loader)\n","  avg_ssim_velocity = total_ssim_velocity / len(test_loader)\n","\n","  avg_mse_pressure = total_mse_pressure / len(test_loader)\n","  avg_psnr_pressure = total_psnr_pressure / len(test_loader)\n","  avg_ssim_pressure = total_ssim_pressure / len(test_loader)\n","\n","  print(f\"Velocity - Average MSE: {avg_mse_velocity:.4f}, Average PSNR: {avg_psnr_velocity:.4f} dB, Average SSIM: {avg_ssim_velocity:.4f}\")\n","  print(f\"Pressure - Average MSE: {avg_mse_pressure:.4f}, Average PSNR: {avg_psnr_pressure:.4f} dB, Average SSIM: {avg_ssim_pressure:.4f}\")"],"metadata":{"id":"7YuP62YP4yCD"},"execution_count":null,"outputs":[]}]}