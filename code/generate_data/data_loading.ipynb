{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP7V2H5J4snROuGt9+0FxmT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import os\n","import torch\n","from torch.utils.data import Dataset, random_split\n","import time"],"metadata":{"id":"utJXl_F2bngr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class FlowFieldDataset(Dataset):\n","    def __init__(self, velocity_dir, pressure_dir, load_all_data=False):\n","        start_time = time.time()\n","\n","        self.velocity_dir = velocity_dir\n","        self.pressure_dir = pressure_dir\n","        self.load_all_data = load_all_data\n","\n","        self.files_true = self.get_joined_files('true_data')\n","        self.files_noisy = self.get_joined_files('noisy_data')\n","\n","        if self.load_all_data:\n","            self.data_true = [self.load_one_instance(f, 'true_data') for f in self.files_true]\n","            self.data_noisy = [self.load_one_instance(f, 'noisy_data') for f in self.files_noisy]\n","\n","        self.load_time = time.time() - start_time\n","        print(f'Dataset Loaded in {self.load_time:.4f} s')\n","\n","    def load_one_instance(self, file_tuple, data_type):\n","        u_file, p_file = file_tuple\n","        u_data = np.load(os.path.join(self.velocity_dir, data_type, u_file))\n","        p_data = np.load(os.path.join(self.pressure_dir, data_type, p_file))\n","\n","        return torch.from_numpy(u_data).float(), torch.from_numpy(p_data).float()\n","\n","    def __len__(self):\n","        return len(self.files_true)\n","\n","    def __getitem__(self, idx):\n","        if self.load_all_data:\n","            u_hr_tensor, p_hr_tensor = self.data_true[idx]\n","            u_lr_tensor, p_lr_tensor = self.data_noisy[idx]\n","        else:\n","            u_hr_tensor, p_hr_tensor = self.load_one_instance(self.files_true[idx], 'true_data')\n","            u_lr_tensor, p_lr_tensor = self.load_one_instance(self.files_noisy[idx], 'noisy_data')\n","\n","        u_hr_tensor = u_hr_tensor.permute(2, 0, 1)\n","        p_hr_tensor = p_hr_tensor.unsqueeze(0)\n","        u_lr_tensor = u_lr_tensor.permute(2, 0, 1)\n","        p_lr_tensor = p_lr_tensor.unsqueeze(0)\n","\n","        return (u_hr_tensor, p_hr_tensor), (u_lr_tensor, p_lr_tensor)\n","\n","    def join_files(self, velocity_files, pressure_files):\n","        joined_files = []\n","        for u_file in velocity_files:\n","            u_index = u_file.split('_')[-1].split('.')[0]\n","            corresponding_p_file = next((p for p in pressure_files if p.split('_')[-1].split('.')[0] == u_index), None)\n","            if corresponding_p_file:\n","                joined_files.append((u_file, corresponding_p_file))\n","        return joined_files\n","\n","    def get_joined_files(self, folder_name):\n","        velocity_files = sorted(os.listdir(os.path.join(self.velocity_dir, folder_name)))\n","        pressure_files = sorted(os.listdir(os.path.join(self.pressure_dir, folder_name)))\n","        return self.join_files(velocity_files, pressure_files)"],"metadata":{"id":"EGZcWkR62LKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_test_split(dataset, test_size=0.2):\n","  total_samples = len(dataset)\n","  test_sample_size = int(test_size * total_samples)\n","  train_sample_size = total_samples - test_sample_size\n","\n","  train_dataset, test_dataset = random_split(dataset, [train_sample_size, test_sample_size])\n","\n","  return train_dataset, test_dataset"],"metadata":{"id":"uCER_YahGVKc"},"execution_count":null,"outputs":[]}]}